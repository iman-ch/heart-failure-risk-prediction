---
title: "Heart Failure Prediction"
output:
  html_document:
    toc: yes
    df_print: paged
  pdf_document:
    fig_width: 6
    fig_height: 4
    toc: yes
    number_sections: yes
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE)
```

### Load Packages and Data

```{r}
library(pROC)
library(caret)
library(ggplot2)
library(dplyr)
library(tidyr)
library(corrplot)
library(PRROC)
library(ggbiplot)
library(factoextra)
library(rpart.plot)

# load data
hfp_data <- read.csv("data/heart_failure_clinical_records_dataset.csv")
```

### 1. Explore Dataset

```{r}
# read structure/summary
str(hfp_data)
summary(hfp_data)
head(hfp_data)

# check missing values
colSums(is.na(hfp_data))

# distribution
table(hfp_data$DEATH_EVENT)
prop.table(table(hfp_data$DEATH_EVENT))
```

```{r}

```

203 (67.8%) of patients survived, while 96 (32.1%) died.

Which variables are related to whether a patient died from heart failure
(response variable (`DEATH_EVENT`)?

### 2. Exploratory Data Analysis

#### 2. 1 Correlation Matrix

```{r, fig.width=8, fig.height=6}
# correlation matrix
cor_matrix_y <- cor(hfp_data)
cor_matrix_y

# top predictors most correlated with DEATH_EVENT
cor_target <- cor_matrix_y["DEATH_EVENT", ]
top_5 <- sort(abs(cor_target[names(cor_target) != "DEATH_EVENT"]), decreasing = TRUE)[1:5]
top_5

# heatmap
corrplot(cor_matrix_y,
         method = "color",       
         type = "upper",         # upper triangle
         order = "hclust",       # group similar variables
         addCoef.col = "black",  # show correlation values
         tl.cex = 0.7,           # axis text
         number.cex = 0.6,       # correlation values
         tl.col = "black",       # axis label color
         cl.cex = 0.6,           # color legend text size
         mar = c(1, 1, 2.5, 1))  # plot margins

title("Correlation Matrix of Variables in Heart Failure Dataset", cex.main = 0.75)
```

From this matrix, we can see that the top 5 predictors most correlated
with `DEATH_EVENT` are `time`, `serum_creatinine`, `ejection_fraction`,
`age`, and `serum_sodium`.

#### 2.2 Boxplots of Continuous Features

To observe visual differences, we explored all continuous variables
grouped by `DEATH_EVENT`.

```{r, fig.width=10, fig.heigh=6}
# ID binary vars and derive continuous ones
binary_vars <- c("sex", "diabetes", "high_blood_pressure", "smoking", "anaemia")
continuous_vars <- setdiff(names(hfp_data), c(binary_vars, "DEATH_EVENT"))

# include all continuous vars + DEATH_EVENT
box_vars <- hfp_data[, c(continuous_vars, "DEATH_EVENT")]

# pivot longer for faceted plotting
long_box <- pivot_longer(box_vars, 
                         cols = -DEATH_EVENT, 
                         names_to = "Variable", 
                         values_to = "Value")

# plot
ggplot(long_box, aes(x = factor(DEATH_EVENT), y = Value, fill = factor(DEATH_EVENT))) +
  geom_boxplot(alpha = 0.6, outlier.color = "red", outlier.alpha = 0.3) +
  facet_wrap(~ Variable, scales = "free", ncol = 3) +
  labs(title = "Boxplots of All Continuous Variables by Death Event",
       x = "Death Event", y = "Value", fill = "Death Event") +
  scale_fill_manual(values = c("0" = "pink", "1" = "turquoise"),
                    labels = c("0 (Survived)", "1 (Died)")) +
  theme_minimal()
```

These boxplots align with the top predictors seen in the correlation
matrix above.

#### 2.3 Violin Plots for Top Predictors

Based on the correlation and boxplots, we've selected the top 5 features
to explore more deeply.

```{r}
top_vars <- c("time", "serum_creatinine", "ejection_fraction", "age", "serum_sodium")

for (var in top_vars) {
  p <- ggplot(hfp_data, aes(x = factor(DEATH_EVENT), y = .data[[var]], fill = factor(DEATH_EVENT))) +
    geom_violin(trim = FALSE, alpha = 0.6) +
    geom_boxplot(width = 0.1, fill = "white") +
    scale_fill_manual(values = c("0" = "pink", "1" = "turquoise"),
                      labels = c("0 (Survived)", "1 (Died)")) +
    labs(title = paste(var, "by Death Event"), x = "Death Event", y = var) +
    theme_minimal()
  print(p)
}
```

Each of these plots shows the value comparision of partients who
survived (`DEATH_EVENT`=0), and died (`DEATH_EVENT`=1)

-   `time`: patients who survived had a longer time where they followed
    up compared to those who died.

-   `serum_creatnine`: shows more right-skewed distributions for
    patients who died, which shows that they had higher levels of serum
    creatnine, an indicator of poor kidney function.

-   `ejection_fraction`: there was more instances of lower ejection
    fraction (how much blood the heart pumps out each heartbeat) in
    patients who died.

-   `age`: the died group skews older, confirming that age is a risk
    factor, but still there is some overlap so it shouldn't be used as a
    sole predictor.

-   `serum_sodium`: the distributions are a bit similar, with slightly
    lower sodium levels in patients who died. Since they are so similar,
    this variable might be weakly predictive.

#### 2.4 Density Plot for Age

We wanted to explore how the top correlated variable (`time`) is
distributed across the response group.

```{r}
ggplot(hfp_data, aes(x = time, fill = factor(DEATH_EVENT))) +
  geom_density(alpha = 0.4, color = NA) +
  labs(
    title = "Time (Follow up Duration) Density by Death Event",
    x = "Time (days)",
    y = "Density",
    fill = "Death Event"
  ) +
  scale_fill_manual(values = c("0" = "pink", "1" = "turquoise"),
                    labels = c("0 (Survived)", "1 (Died)")) +
  theme_minimal()


```

This plot shows the estimated probability density of follow up-time (in
days). The blue curve (deaths) is heavily concentrated at lower times
(\<50 days), and suggests that patients who died had shorter follow up
durations. The pink curve shows a "bimodal" distribution (two peaks at
\~75 and 200), and indicates a wider range and longer time duration
among surviving patients. This feature may contribute more significantly
to class seperation and therefore reduce error training rate in models
like LDA or Logistic Regression.

### 3. Preprocessing

#### 3.1 Scaling Predictors

```{r}
binary_vars <- c("sex", "diabetes", "high_blood_pressure", "smoking", "anaemia")
continuous_vars <- setdiff(names(hfp_data), c(binary_vars, "DEATH_EVENT"))

scaled_cont <- scale(hfp_data[, continuous_vars])
binary_data <- hfp_data[, binary_vars]

hfp_scaled <- cbind(as.data.frame(scaled_cont), binary_data, DEATH_EVENT = hfp_data$DEATH_EVENT)

head(hfp_scaled)
```

All continuous features were standardized using z-score scaling (mean =
0, sd = 1). This makes sure that the features have equal weight in
distance-based methods. The binary features were kept the same to keep
their categorical interpretation.

#### 3.2 Train-Test Split

```{r}
hfp_data$DEATH_EVENT <- factor(hfp_data$DEATH_EVENT, levels = c(0, 1), labels = c("Survived", "Died"))

set.seed(2025)

train_index <- createDataPartition(hfp_scaled$DEATH_EVENT, p = 0.7, list = FALSE)
train_data <- hfp_scaled[train_index, ]
test_data <- hfp_scaled[-train_index, ]
```

The dataset was split 70/30 using stratified sampling, which makes sure
that the data distribution is consistent, keeping the same amount of
instances where patients have died for the training and test set.

#### 3.3 K-Fold Cross Validation

```{r}
train_control <- trainControl(method="cv", number=10, classProbs = TRUE, summaryFunction = twoClassSummary, savePredictions = TRUE)
```

10-fold cross validation was set up for all models.

### 4. Model Training

We trained the following classifiers:

-   Quadratic Discriminant Analysis (QDA)

-   Linear Discriminant Analysis (LDA)

-   Logistic Regression (LogReg)

-   k-Nearest Neighbors (KNN)

-   Support Vector Machine (SVM)

-   Random Forest (RF)

-   Decision Tree (DT)

Since there are binary values, we performed a conversion in the
beginning to designate values for patients who died and survived.

```{r}
#Classfication Models
# Convert DEATH_EVENT to a factor for classification since it is numeric (meant for regression)
#test_data$DEATH_EVENT <- as.factor(test_data$DEATH_EVENT)
#train_data$DEATH_EVENT <- as.factor(train_data$DEATH_EVENT)

test_data$DEATH_EVENT <- factor(test_data$DEATH_EVENT, levels = c(0, 1), labels = c("Survived", "Died"))
train_data$DEATH_EVENT <- factor(train_data$DEATH_EVENT, levels = c(0, 1), labels = c("Survived", "Died"))

# QDA Model
set.seed(2025)
qda_model <- train(DEATH_EVENT ~ ., data = train_data, method = "qda", trControl = train_control, metric = "ROC")
print(qda_model)

# LDA Model
lda_model <- train(DEATH_EVENT ~ ., data = train_data, method = "lda", trControl = train_control, metric = "ROC")
print(lda_model)

# Logistic Regression
logistic_regression_model <- train(DEATH_EVENT ~ ., data = train_data, method = "glm", family = "binomial", trControl = train_control, metric = "ROC")
print (logistic_regression_model)

# KNN
knn_model <- train(DEATH_EVENT ~ ., data = train_data, method = "knn", trControl = train_control, metric = "ROC")
print (knn_model)

# SVM
svm_model <- train(DEATH_EVENT ~ ., data = train_data, method = "svmRadial", trControl = train_control, preProcess = c("center", "scale"), metric = "ROC") 
print(svm_model)

# Random Forest
rf_model <- train(DEATH_EVENT ~ ., data = train_data, method = "rf", trControl = train_control, metric = "ROC")
print(rf_model)

# Decision Tree
dt_model <- train(DEATH_EVENT ~ ., data = train_data, method = "rpart", trControl = train_control, metric = "ROC")
print(dt_model)

rpart.plot(dt_model$finalModel, main = "Decision Tree for Heart Failure Prediction")
dt_pred <- predict(dt_model, newdata = test_data)
confusionMatrix(dt_pred, test_data$DEATH_EVENT)

```

### 5. Model Evaluation: F1 and AUC

#### 5.1 Utility Functions

```{r}
# Function to calculate F1 from confusion matrix
calculate_f1 <- function(cm) {
  precision <- cm$byClass["Pos Pred Value"]
  recall <- cm$byClass["Sensitivity"]
  
  # Handle division by zero
  if ((precision + recall) == 0) {
    return(NA)
  }
  f1 <- 2 * (precision * recall) / (precision + recall)
  return(round(f1, 4))
}

calculate_auc <- function(actual, predicted_probs, positive_label = "Died") {
  actual <- factor(actual, levels = c("Survived", "Died"))
  predicted_probs <- as.numeric(predicted_probs)
  roc_curve <- roc(response = actual, predictor = predicted_probs, levels = c("Survived", "Died"))
  auc_value <- auc(roc_curve)
  return(round(auc_value, 4))
}
```

#### 5.2 Metrics per Model

We generated confusion matrices for each of the models, along with
manually calculating F1-score and AUC.

```{r}

# Logistic Regression
logreg_pred <- predict(logistic_regression_model, newdata = test_data)
logreg_cm <- confusionMatrix(logreg_pred, test_data$DEATH_EVENT, positive = "Died")
logreg_f1 <- calculate_f1(logreg_cm)
print(logreg_cm)
cat("\nF1: ", logreg_f1, "\n") #Calculating F1
logreg_probs <- predict(logistic_regression_model, newdata = test_data, type = "prob")
auc_value <- calculate_auc(test_data$DEATH_EVENT, logreg_probs$Died) #Calculating AUC
cat("AUC:", auc_value, "\n\n\n")


# LDA
lda_pred <- predict(lda_model, newdata = test_data)
lda_cm <- confusionMatrix(lda_pred, test_data$DEATH_EVENT, positive = "Died")
lda_f1 <- calculate_f1(lda_cm)
print(lda_cm)
cat("\nF1: ", lda_f1, "\n")
lda_probs <- predict(lda_model, newdata = test_data, type = "prob")
auc_value <- calculate_auc(test_data$DEATH_EVENT, lda_probs$Died) #Calculating AUC
cat("AUC:", auc_value, "\n\n\n")



# QDA
qda_pred <- predict(qda_model, newdata = test_data)
qda_cm <- confusionMatrix(qda_pred, test_data$DEATH_EVENT, positive = "Died")
qda_f1 <- calculate_f1(qda_cm)
print(qda_cm)
cat("\nF1: ", qda_f1, "\n")
qda_probs <- predict(qda_model, newdata = test_data, type = "prob")
auc_value <- calculate_auc(test_data$DEATH_EVENT, qda_probs$Died) #Calculating AUC
cat("AUC:", auc_value, "\n\n\n")


# KNN
knn_pred <- predict(knn_model, newdata = test_data)
knn_cm <- confusionMatrix(knn_pred, test_data$DEATH_EVENT, positive = "Died")
knn_f1 <- calculate_f1(knn_cm)
print(knn_cm)
cat("\nF1: ", knn_f1, "\n")
knn_probs <- predict(knn_model, newdata = test_data, type = "prob")
auc_value <- calculate_auc(test_data$DEATH_EVENT, knn_probs$Died) #Calculating AUC
cat("AUC:", auc_value, "\n\n\n")


# SVM
svm_pred <- predict(svm_model, newdata = test_data)
svm_cm <- confusionMatrix(svm_pred, test_data$DEATH_EVENT, positive = "Died")
svm_f1 <- calculate_f1(svm_cm)
print(svm_cm)
cat("\nF1: ", svm_f1, "\n")
svm_probs <- predict(svm_model, newdata = test_data, type = "prob")
auc_value <- calculate_auc(test_data$DEATH_EVENT, svm_probs$Died)  #Calculating AUC
cat("AUC:", auc_value, "\n\n\n")


# Random Forest
rf_pred <- predict(rf_model, newdata = test_data)
rf_cm <- confusionMatrix(rf_pred, test_data$DEATH_EVENT, positive = "Died")
rf_f1 <- calculate_f1(rf_cm)
print(rf_cm)
cat("\nF1: ", rf_f1, "\n")
rf_probs <- predict(rf_model, newdata = test_data, type = "prob")
auc_value <- calculate_auc(test_data$DEATH_EVENT, rf_probs$Died) #Calculating AUC
cat("AUC:", auc_value, "\n\n\n")


# Decision Tree
dt_pred <- predict(dt_model, newdata = test_data)
dt_cm <- confusionMatrix(dt_pred, test_data$DEATH_EVENT, positive = "Died")
dt_f1 <- calculate_f1(dt_cm)
print(dt_cm)
cat("\nF1: ", dt_f1, "\n")
dt_probs <- predict(dt_model, newdata = test_data, type = "prob")
auc_value <- calculate_auc(test_data$DEATH_EVENT, dt_probs$Died) #Calculating AUC
cat("AUC:", auc_value, "\n\n\n")
```

#### 5.3 Variable Importance for Random Forest

```{r}
# variable importance for rf
library(caret)
importance_rf <- varImp(rf_model)
print(importance_rf)

# top 10 important variables
plot(importance_rf, top = 10, main = "Top 10 Important Variables - Random Forest")
```

varImp() IDs which predictors had the biggest impact on model
performance. In RF, var. importance is calculated based on how much each
variable reduces impurity across all trees. The top variables are
`time`, `serum_creatine`, `ejection_fraction`, and `age`, which aligns
with domain knowledge-\> indicators of heart failure progression.

### 6. Performance Comparision

#### 6.1 Summary

```{r}
results <- resamples(list(SVM = svm_model, LogReg = logistic_regression_model, LDA = lda_model, QDA = qda_model, RF = rf_model, KNN = knn_model))
summary(results)

```

#### 6.2 Comparison Plots (Barplot and Boxplot of Model Performace)

```{r}
# Boxplot for Accuracy
## bwplot(results, metric = "Accuracy")
get_accuracy <- function(cm) {
  accuracy <- as.numeric(cm$overall["Accuracy"])
  return(round(accuracy, 4))
}

logreg_acc <- get_accuracy(logreg_cm)
lda_acc <- get_accuracy(lda_cm)
qda_acc <- get_accuracy(qda_cm)
knn_acc <- get_accuracy(knn_cm)
svm_acc <- get_accuracy(svm_cm)
rf_acc <- get_accuracy(rf_cm)
dt_acc <- get_accuracy(dt_cm)

accuracy_df <- data.frame(Model = c("LogReg", "LDA", "QDA", "KNN", "SVM","RF", "DT"),
                          Accuracy = c(logreg_acc, lda_acc, qda_acc, knn_acc, svm_acc, rf_acc, dt_acc))

# Bar plot for accuracy - used this instead of boxplot because the accuracy were singe points so boxplot was a line rather than box
barplot(accuracy_df$Accuracy, names.arg = accuracy_df$Model, col = "darkgreen",
        main = "Accuracy Comparison of Models", ylab = "Accuracy")

# Boxplot for F1 Score 
f1_results <- data.frame(Model = c("Logistic Regression", "LDA", "QDA", "KNN", "SVM", "Random Forest", "Decision Tree"),
                         F1_Score = c(logreg_f1, lda_f1, qda_f1, knn_f1, svm_f1, rf_f1, dt_f1))
# boxplot(F1_Score ~ Model, data = f1_results, col = "lightblue",
#         main = "F1 Score Comparison of Models", ylab = "F1 Score")


# Boxplot for AUC (ROC)
bwplot(results, metric = "ROC")
```

#### 6.3 ROC Curve

```{r}
# function to extract ROC curve data
extract_roc_df <- function(roc_obj, model_name) {
  data.frame(
    FPR = 1 - roc_obj$specificities,
    TPR = roc_obj$sensitivities,
    Model = model_name
  )
}

true_labels <- test_data$DEATH_EVENT

# calculate ROC curves and AUCs
roc_logreg <- roc(true_labels, logreg_probs$Died)
roc_knn    <- roc(true_labels, knn_probs$Died)
roc_svm    <- roc(true_labels, svm_probs$Died)
roc_rf     <- roc(true_labels, rf_probs$Died)

# get AUC values
auc_logreg <- round(auc(roc_logreg), 3)
auc_knn    <- round(auc(roc_knn), 3)
auc_svm    <- round(auc(roc_svm), 3)
auc_rf     <- round(auc(roc_rf), 3)

# combine data
roc_data <- rbind(
  extract_roc_df(roc_logreg, paste0("Logistic (AUC = ", auc_logreg, ")")),
  extract_roc_df(roc_knn, paste0("KNN (AUC = ", auc_knn, ")")),
  extract_roc_df(roc_svm, paste0("SVM (AUC = ", auc_svm, ")")),
  extract_roc_df(roc_rf, paste0("Random Forest (AUC = ", auc_rf, ")"))
)


# plot
ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
  geom_line(size = 0.6) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey") +
  labs(title = "ROC Curves for All Models",
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11)
  )


```

AUC shows probability that random positive class instance (Died) is
ranked higher than random negative class (Survived) in predicted
probability. It is more informative when class distribution is
imbalanced (ex. `hfp_data` 68/32) in comparison to accuracy and F1-score
since they are threshold-independent and not affected by skewed data.

-   **Random Forest** performs the best (AUC=0.952), and the top-left
    curve indicates a high sensitive and low false positive rate
-   **Logistic Regression** and **Support Vector Machine** perform
    strongly (AUC=0.903, 0.886 respectively)
-   **K-Nearest Neighbour** is least effective out of the 4 (AUC=0.814).
    This might be because of noise, high variance, "curse of
    dimensionality".

#### 6.4 PR Curves

```{r}
# PR curves
pr_logreg <- pr.curve(scores.class0 = logreg_probs$Died[true_labels == "Died"],
                      scores.class1 = logreg_probs$Died[true_labels == "Survived"],
                      curve = TRUE)

pr_knn <- pr.curve(scores.class0 = knn_probs$Died[true_labels == "Died"],
                   scores.class1 = knn_probs$Died[true_labels == "Survived"],
                   curve = TRUE)

pr_svm <- pr.curve(scores.class0 = svm_probs$Died[true_labels == "Died"],
                   scores.class1 = svm_probs$Died[true_labels == "Survived"],
                   curve = TRUE)

pr_rf <- pr.curve(scores.class0 = rf_probs$Died[true_labels == "Died"],
                  scores.class1 = rf_probs$Died[true_labels == "Survived"],
                  curve = TRUE)

# extract PR curve as data frame
extract_pr_df <- function(pr_obj, model_name) {
  data.frame(
    Recall = pr_obj$curve[, 1],
    Precision = pr_obj$curve[, 2],
    Model = model_name
  )
}

# PR AUCs
auc_pr_logreg <- round(pr_logreg$auc.integral, 3)
auc_pr_knn    <- round(pr_knn$auc.integral, 3)
auc_pr_svm    <- round(pr_svm$auc.integral, 3)
auc_pr_rf     <- round(pr_rf$auc.integral, 3)

# combine to one df for ggplot
pr_data <- rbind(
  extract_pr_df(pr_logreg, paste0("Logistic (PR AUC = ", auc_pr_logreg, ")")),
  extract_pr_df(pr_knn, paste0("KNN (PR AUC = ", auc_pr_knn, ")")),
  extract_pr_df(pr_svm, paste0("SVM (PR AUC = ", auc_pr_svm, ")")),
  extract_pr_df(pr_rf, paste0("Random Forest (PR AUC = ", auc_pr_rf, ")"))
)

# plot
ggplot(pr_data, aes(x = Recall, y = Precision, color = Model)) +
  geom_line(size = 0.6) +
  labs(title = "Precision-Recall Curves for All Models",
       x = "Recall (Sensitivity)",
       y = "Precision",
       color = "Model") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    axis.text = element_text(size = 10),
    axis.title = element_text(size = 11)
  )

```

PR curve focuses on positive class ("Died") by plotting precision vs.
recall. More informative than ROC AUC when class distribution is
imbalanced, since it directly evaluates model ability to identify true
positives.

-   **Random Forest** has highest AUC-PR (0.912) which shows its ability
    to identify actual deaths while also keeping high precision.
-   **Logistic Regression** had strong performance at 0.849
-   **Support Vector Machine** gives a moderate performance at 0.8
-   **K-Nearest Neighbour** performed least well, at 0.746, showing it
    had more false positives and less effective in ID actual deaths.

### 7. Hyperparameter Tuning

#### 7.1 K-Nearest Neighbours (KNN)

```{r}
knn_grid <- expand.grid(k = seq(3, 21, 2))
knn_model <- train(DEATH_EVENT ~ ., data = train_data, method = "knn",
                   trControl = train_control, metric = "ROC",
                   tuneGrid = knn_grid)
print(knn_model)

```

#### 7.2 Support Vector Machine (SVM)

```{r}
svm_grid <- expand.grid(C = 2^(-2:2), sigma = 2^(-2:2))
svm_model <- train(DEATH_EVENT ~ ., data = train_data, method = "svmRadial",
                   trControl = train_control, metric = "ROC",
                   preProcess = c("center", "scale"),
                   tuneGrid = svm_grid)
print(svm_model)

```

#### 7.3 Random Forest (RF)

```{r}
rf_grid <- expand.grid(mtry = 2:6)
rf_model <- train(DEATH_EVENT ~ ., data = train_data, method = "rf",
                  trControl = train_control, metric = "ROC",
                  tuneGrid = rf_grid)
print(rf_model)

```

#### 7.4 Decision Tree (DT)

```{r}
dt_grid <- expand.grid(cp = seq(0.001, 0.1, by = 0.005))
dt_model <- train(DEATH_EVENT ~ ., data = train_data, method = "rpart",
                  trControl = train_control, metric = "ROC",
                  tuneGrid = dt_grid)
print(dt_model)

```

For each model trained, we performed grid search and 10-fold cross
validation to optimize `k`, `c`/`sigma`, `mtry`, and `cp` for their
respective models. ROC was used to select optimal model using largest
value.

### 8. Principal Component Analysis (PCA)

We applied PCA to reduce dimensionality so we can plot and interpret
better, as PCA will give us a low-dimensional representation of the data
that captures as much variance as possible.

```{r}
# extract only scaled continuous predictors
pca_data <- hfp_scaled[, continuous_vars]
pca_result <- prcomp(pca_data, center = FALSE, scale. = FALSE)
```

#### 8.1 Scree Plot

This plot shows how much variance is explained by each principal
component.

```{r}
# scree plot -> proportion of variance explained
screeplot(pca_result, type = "lines", main = "Scree Plot of Principal Components")
```

scree plot: shows amt of variance explained by each PC, each point
represents 1 PC and y-axis shows how much variance PC explains. We used
it to determine how many PCs show meaningful dimensionality reduction -
PC1 and PC2 explain most variance.

#### 8.2 PCA Biplot

This plot shows patient scores on the first two PCs and the loadings of
each variable.

```{r}
# biplot
fviz_pca_biplot(pca_result,
                label = "var",                    
                habillage = hfp_scaled$DEATH_EVENT,
                addEllipses = TRUE,             
                col.var = "black",              
                col.ind = "gray",               
                pointsize = 1,                  
                repel = TRUE,                  
                labelsize = 2,                 
                title = "Principal Component Analysis Biplot: Heart Failure Patients") 
```

This biplot visualizes both PCs (Dim1, Dim2) for each patient, and
variable loadings. A positive loading shows that the variable increases
the PC score, and a negative loading shows that the variable decreases
the PC score. Each arrow represents a variable and coneys:

-   direction: how variable contributes to PCs

-   length: how strongly a variable influences PCA (longer=stronger)

-   angle between arrows:

    -   same direction = **positive correlation** between variables

    -   opposite direction = **negative correlation** between variables

    -   perpendicular = uncorrelated

This helps us visually see the clutering between survived (0) and died
(1).

We can see that variables like `serum_creatine` and `age` arrows are
**long and point right**, therfore they strongly contribute to PC1.

`ejection_fraction` and `serum_sodium` point towards the **upper-left**
quadrant of the plot, and it has moderate positive loading on PC2, but
negative on PC1.

`time` points **bottom-left,** meaning it loads negatively on PC1 and
PC2.

`creatine_phosphokinase` points **downward**, and it contributes mostly
negatively to PC2

`platlets` points **left**, indicating a negative PC1 loading

#### 8.3 PC1 vs. PC2 Scatterplot

This plot visualizes patients in the PCA space, colored by survival
outcome.

```{r}
# PCA dataframe
pca_df <- as.data.frame(pca_result$x)
pca_df$DEATH_EVENT <- hfp_scaled$DEATH_EVENT         

# PC1 vs. PC2
ggplot(pca_df, aes(x = PC1, y = PC2, color = DEATH_EVENT)) +
  geom_point(alpha = 0.6) +
  labs(title = "Principal Component Analysis: PC1 vs. PC2", x = "PC1", y = "PC2", color = "Death Event") +
  theme_minimal()
```

The scatterplot explores how individual points lie in PCA space. It
shows the spatial grouping of `DEATH_EVENT` classes, but it is not
perfectly seperable. It also shows that PCA doesn't fully separate
classes but shows patterns instead.

### 9. K-Means Clustering

We applied K-Means (k=2) to identify groups in PCA reduced space, this
will explore **unsupervised** grouping.

```{r}
set.seed(2025)

# top 2 PCs for visualization, full data for clustering
kmeans_result <- kmeans(pca_data, centers = 2, nstart = 25)

# cluster labels to PCA df
pca_df$Cluster <- as.factor(kmeans_result$cluster)

# confusion matrix to check alignment with DEATH_EVENT
table(Cluster = pca_df$Cluster, DEATH_EVENT = pca_df$DEATH_EVENT)

# plot clusters
ggplot(pca_df, aes(x = PC1, y = PC2, color = Cluster)) +
  geom_point(alpha = 0.6) +
  labs(title = "K-Means Clustering (k = 2) on PCA Components") +
  theme_minimal()

```

We used k-means (k=2) on first 2 PC to see the unsupervised grouping of
patients. Compared to the other scatter plot above which used
`DEATH_EVENT`, K-Means formed the groups based only on patterns in the
data and no knowledge of survival outcomes. The plot shows two distinct
clusters, which indicates the dataset has a natural structure, and the
patients are grouped based on similar features (ex. `age`,
`serum_creatine`, etc) which had a strong impact on patterns found by
PCA (plots above).
